





import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import  OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import r2_score






dataset = pd.read_csv("https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Machine+Learning+Supervis%C3%A9/R%C3%A9gression+r%C3%A9gularis%C3%A9es/gene+data/data_lasso.csv",)


dataset.head()





dataset = dataset.drop(dataset.columns[[0,1]], axis=1)

dataset.head()








shape = dataset.shape

print("Number of row ", shape[0])
print("Number of columns ", shape[1])

# Nombre de ligne inf à nombre de colonne





# tester s'il existe des valeurs nulles
# dataset.describe(include='all')
# il n'y pas de valeur nulles
(dataset.isna().sum() / dataset.shape[0])*100
print("\n2. Colonnes avec des nulls:")
print(dataset.columns[dataset.isnull().any()].tolist())





target_variable = "target"

X = dataset.drop(target_variable, axis = 1)
Y = dataset.loc[:,target_variable]
print("Y", Y.head())
print("X", X.head())





print("Dividing into train and test sets...")
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
print("...Done.")
print()





# Comment on fait: d'abord verifier les types de variable: discrete,continue, nominal, ordinal
colonnes_string = dataset.select_dtypes(include=['object']).columns.tolist()
print("Colonnes string:", colonnes_string)

# comme les VA sont des float, alors Standardisation, Normalisation
scaler = StandardScaler()



# Preprocessings on train set
print("Performing preprocessings on train set...")
#print(X_train.head())
X_train = scaler.fit_transform(X_train)
print('...Done.')
type(X_train)
print(X_train[0:5]) # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore
print()

# Preprocessings on test set
print("Performing preprocessings on test set...")
#print(X_test.head())
X_test = scaler.transform(X_test) # Don't fit again !!
print('...Done.')
print(X_test[0:5,:]) # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore
print()








# Perform 3-fold cross-validation to evaluate the generalized R2 score obtained with a Ridge model
# print("3-fold cross-validation...")
# regressor = Ridge()
# scores = cross_val_score(regressor, X_train, Y_train, cv=3)
# print('The cross-validated R2-score is : ', scores.mean())
# print('The standard deviation is : ', scores.std())

# & :penalité

ridge1 = Ridge()
print(ridge1)
ridge1.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", ridge1.score(X_train, Y_train))
print("R2 score on test set : ", ridge1.score(X_test, Y_test))





ridge2 = Ridge(alpha=1000000)
print(ridge2)
ridge2.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", ridge2.score(X_train, Y_train))
print("R2 score on test set : ", ridge2.score(X_test, Y_test))





ridge3 = Ridge(alpha=100000000)
print(ridge3)
ridge3.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", ridge3.score(X_train, Y_train))
print("R2 score on test set : ", ridge3.score(X_test, Y_test))








data_dict = {
    'Feature': X.columns,
    'Ridge1': ridge1.coef_,
    'Ridge2': ridge2.coef_,
    'Ridge3': ridge3.coef_
            }

coefficients_ridge = pd.DataFrame(data = data_dict)
coefficients_ridge.head()





import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio

fig = px.line(coefficients_ridge, x = 'Feature', y = ['Ridge1', 'Ridge2', 'Ridge3'])
fig.show()






from sklearn.linear_model import Ridge, Lasso


lasso1 = Lasso(alpha = 1)
print(lasso1)
lasso1.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", lasso1.score(X_train, Y_train))
print("R2 score on test set : ", lasso1.score(X_test, Y_test))





lasso2 = Lasso(alpha = 30)
print(lasso2)
lasso2.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", lasso2.score(X_train, Y_train))
print("R2 score on test set : ", lasso2.score(X_test, Y_test))






lasso3 = Lasso(alpha = 100)
print(lasso3)
lasso3.fit(X_train, Y_train)
# Print R^2 scores
print("R2 score on training set : ", lasso3.score(X_train, Y_train))
print("R2 score on test set : ", lasso3.score(X_test, Y_test))






data_dict = {
    'Feature': X.columns,
    'Lasso1': lasso1.coef_,
    'Lasso2': lasso2.coef_,
    'Lasso3': lasso3.coef_
            }

coefficients_lasso = pd.DataFrame(data = data_dict)
coefficients_lasso.head()



fig = px.line(coefficients_lasso, x = 'Feature', y = ['Lasso1', 'Lasso2', 'Lasso3'])
fig.show()





# Perform grid search
print("Grid search...")
regressor = Ridge()
# Grid of values to be tested
params = {
    'alpha': [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]
}
best_ridge = GridSearchCV(regressor, param_grid = params, cv = 5) # cv : the number of folds to be used for CV
best_ridge.fit(X_train, Y_train)
print("...Done.")
print("Best hyperparameters : ", best_ridge.best_params_)
print("Best R2 score : ", best_ridge.best_score_)



# Perform grid search
print("Grid search...")
regressor = Lasso()
# Grid of values to be tested
params = {
    'alpha': [1, 2, 3, 5, 10, 20, 30]
}
best_lasso = GridSearchCV(regressor, param_grid = params, cv = 5) # cv : the number of folds to be used for CV
best_lasso.fit(X_train, Y_train)
print("...Done.")
print("Best hyperparameters : ", best_lasso.best_params_)
print("Best R2 score : ", best_lasso.best_score_)






# Print R^2 scores
print("RIDGE / R2 score on training set : ", best_ridge.score(X_train, Y_train))
print("RIDGE / R2 score on test set : ", best_ridge.score(X_test, Y_test))
print()
print("LASSO / R2 score on training set : ", best_lasso.score(X_train, Y_train))
print("LASSO / R2 score on test set : ", best_lasso.score(X_test, Y_test))






data_dict = {
    'Feature': X.columns,
    'Best_Ridge': best_ridge.best_estimator_.coef_,
    'Best_Lasso': best_lasso.best_estimator_.coef_
            }

coefficients = pd.DataFrame(data = data_dict)
coefficients.head()



fig = px.line(coefficients, x = 'Feature', y = ['Best_Ridge', 'Best_Lasso'])
fig.show()






mask = coefficients['Best_Lasso'] != 0
best_features = coefficients.loc[mask, 'Feature'].to_list()
best_features






X = X.loc[:, best_features]
X.head()



# Divide dataset Train set & Test set
print("Dividing into train and test sets...")
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
print("...Done.")
print()



# Create scaler for numeric features
scaler = StandardScaler()

# Preprocessings on train set
print("Performing preprocessings on train set...")
print(X_train.head())
X_train = scaler.fit_transform(X_train)
print('...Done.')
print(X_train[0:5]) # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore
print()

# Preprocessings on test set
print("Performing preprocessings on test set...")
print(X_test.head())
X_test = scaler.transform(X_test) # Don't fit again !!
print('...Done.')
print(X_test[0:5,:]) # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore
print()






from sklearn.linear_model import LinearRegression
# Train linear regression (no regularization)
print("Train model...")
regressor = LinearRegression()
regressor.fit(X_train, Y_train)
print("...Done.")



# Print R^2 scores
print("R2 score on training set : ", regressor.score(X_train, Y_train))
print("R2 score on test set : ", regressor.score(X_test, Y_test))




