








# 1. IMPORT Regression
# ðŸ“Š 1. RÃ‰GRESSION (PrÃ©dire un NOMBRE)
# Quand ?
# PrÃ©dire valeur continue : prix, tempÃ©rature, quantitÃ©, Ã¢ge...
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# 2. PRÃ‰PARATION DES DONNÃ‰ES
# - GÃ©rer valeurs manquantes
df.fillna(df.mean(), inplace=True)

# - Encoder variables catÃ©gorielles (si nÃ©cessaire)
# Pour rÃ©gression : souvent LabelEncoder ou OneHotEncoder
le = LabelEncoder()
df['ville'] = le.fit_transform(df['ville'])

# OU OneHotEncoder pour crÃ©er colonnes binaires
df_encoded = pd.get_dummies(df, columns=['ville'], drop_first=True)

# - SÃ©parer X et y
X = df.drop('prix', axis=1)  # Features
y = df['prix']                # Target (variable continue)

# 3. SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. SCALING (Important pour rÃ©gression !)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Attention : transform seulement !

# 5. ENTRAÃŽNEMENT
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# 6. PRÃ‰DICTION
y_pred = model.predict(X_test_scaled)

# 7. Ã‰VALUATION
print(f"RÂ² : {r2_score(y_test, y_pred):.3f}")  # 0 Ã  1 (proche de 1 = bon)
print(f"RMSE : {mean_squared_error(y_test, y_pred, squared=False):.2f}")
print(f"MAE : {mean_absolute_error(y_test, y_pred):.2f}")





# 1. IMPORT
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# 2-3. PRÃ‰PARATION et SPLIT (identique Ã  classification)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. SCALING (OBLIGATOIRE pour Logistic Regression !)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. ENTRAÃŽNEMENT
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train)

# 6. PRÃ‰DICTION
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]  # ProbabilitÃ©s classe 1

# 7. Ã‰VALUATION
print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"AUC-ROC : {roc_auc_score(y_test, y_proba):.3f}")  # SpÃ©cifique logistique
print(classification_report(y_test, y_pred))


## ðŸŒ³ ARBRES DE DÃ‰CISION & FORÃŠTS ALÃ‰ATOIRES
### 1. ARBRE DE DÃ‰CISION (Decision Tree)
Concept simple
C'est comme un organigramme de questions qui divise les donnÃ©es Ã©tape par Ã©tape.
Exemple concret : PrÃ©dire si quelqu'un va acheter un produit
                    Ã‚ge < 30 ans ?
                   /              \
                OUI                NON
                 /                  \
        Salaire > 2000â‚¬ ?      MariÃ© ?
           /        \           /     \
        OUI        NON        OUI    NON
         /          \          /       \
    ACHÃˆTE    ACHÃˆTE PAS   ACHÃˆTE  ACHÃˆTE PAS






from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(
    max_depth=5,           # Profondeur max (limite overfitting) â­
    min_samples_split=10,  # Min Ã©chantillons pour diviser nÅ“ud
    min_samples_leaf=5,    # Min Ã©chantillons par feuille
    max_features='sqrt',   # Nombre features Ã  considÃ©rer
    random_state=42
)
```

**ParamÃ¨tres clÃ©s Ã  retenir** :
- **max_depth** : Plus important ! Limite la profondeur (3-10 gÃ©nÃ©ralement)
- **min_samples_split** : Ã‰vite divisions sur petits groupes
- **min_samples_leaf** : Ã‰vite feuilles trop spÃ©cifiques

---









from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,      # Nombre d'arbres (100-500) â­â­â­
    max_depth=10,          # Profondeur max de chaque arbre â­
    min_samples_split=5,   # Min Ã©chantillons pour diviser
    min_samples_leaf=2,    # Min Ã©chantillons par feuille
    max_features='sqrt',   # Features alÃ©atoires par division â­
    random_state=42,
    n_jobs=-1              # Utilise tous les CPU (parallÃ©lisation)
)


ParamÃ¨tres clÃ©s Ã  retenir :

n_estimators : Plus = mieux (mais plus lent). Start avec 100.
max_depth : ContrÃ´le overfitting (None par dÃ©faut, mais limiter Ã  10-20 souvent mieux)
max_features : 'sqrt' pour classification, 'log2' alternative


# ========== IMPORTS ==========
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# ========== DONNÃ‰ES EXEMPLE ==========
# CrÃ©ons un dataset simple
from sklearn.datasets import load_iris
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ========== 1. ARBRE DE DÃ‰CISION ==========
print("=" * 50)
print("ARBRE DE DÃ‰CISION")
print("=" * 50)

# ModÃ¨le
dt_model = DecisionTreeClassifier(
    max_depth=3,              # Limite profondeur (Ã©vite overfitting)
    min_samples_split=5,
    random_state=42
)

# EntraÃ®nement
dt_model.fit(X_train, y_train)

# PrÃ©diction
y_pred_dt = dt_model.predict(X_test)

# Ã‰valuation
print(f"Accuracy : {accuracy_score(y_test, y_pred_dt):.3f}")
print(f"F1-Score : {f1_score(y_test, y_pred_dt, average='weighted'):.3f}")

# Cross-validation pour vÃ©rifier overfitting
cv_scores_dt = cross_val_score(dt_model, X_train, y_train, cv=5)
print(f"Cross-Val Accuracy : {cv_scores_dt.mean():.3f} (+/- {cv_scores_dt.std():.3f})")

# Visualiser l'arbre (optionnel)
plt.figure(figsize=(15, 10))
plot_tree(dt_model, 
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=10)
plt.title("Arbre de DÃ©cision")
plt.tight_layout()
plt.savefig('decision_tree.png')
print("Arbre sauvegardÃ© dans 'decision_tree.png'")

# ========== 2. FORÃŠT ALÃ‰ATOIRE ==========
print("\n" + "=" * 50)
print("FORÃŠT ALÃ‰ATOIRE")
print("=" * 50)

# ModÃ¨le
rf_model = RandomForestClassifier(
    n_estimators=100,         # 100 arbres
    max_depth=5,              # Profondeur max
    min_samples_split=5,
    max_features='sqrt',      # Features alÃ©atoires
    random_state=42,
    n_jobs=-1                 # ParallÃ©lisation
)

# EntraÃ®nement
rf_model.fit(X_train, y_train)

# PrÃ©diction
y_pred_rf = rf_model.predict(X_test)

# Ã‰valuation
print(f"Accuracy : {accuracy_score(y_test, y_pred_rf):.3f}")
print(f"F1-Score : {f1_score(y_test, y_pred_rf, average='weighted'):.3f}")

# Cross-validation
cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5)
print(f"Cross-Val Accuracy : {cv_scores_rf.mean():.3f} (+/- {cv_scores_rf.std():.3f})")

# Rapport dÃ©taillÃ©
print("\nClassification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf, target_names=iris.target_names))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

# ========== 3. IMPORTANCE DES FEATURES ==========
print("\n" + "=" * 50)
print("IMPORTANCE DES FEATURES (Random Forest)")
print("=" * 50)

feature_importance = pd.DataFrame({
    'Feature': iris.feature_names,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print(feature_importance)

# Visualiser importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Importance')
plt.title('Importance des Features')
plt.tight_layout()
plt.savefig('feature_importance.png')
print("\nGraphique sauvegardÃ© dans 'feature_importance.png'")

# ========== 4. COMPARAISON ==========
print("\n" + "=" * 50)
print("COMPARAISON")
print("=" * 50)

comparison = pd.DataFrame({
    'ModÃ¨le': ['Decision Tree', 'Random Forest'],
    'Accuracy': [
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_rf)
    ],
    'F1-Score': [
        f1_score(y_test, y_pred_dt, average='weighted'),
        f1_score(y_test, y_pred_rf, average='weighted')
    ],
    'CV Mean': [cv_scores_dt.mean(), cv_scores_rf.mean()],
    'CV Std': [cv_scores_dt.std(), cv_scores_rf.std()]
})

print(comparison)








# 1. Toujours faire cross-validation pour dÃ©tecter overfitting
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV: {scores.mean():.3f} (+/- {scores.std():.3f})")

# 2. GridSearchCV pour trouver meilleurs paramÃ¨tres
from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}
grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)
print(f"Meilleurs params: {grid.best_params_}")

# 3. Feature importance pour feature selection
importances = rf_model.feature_importances_
# Garder seulement features importantes (> seuil)
important_features = X.columns[importances > 0.05]
