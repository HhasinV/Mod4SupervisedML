





!pip install --upgrade pyarrow scikit-learn


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay, classification_report

import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
# setting Jedha color palette as default
pio.templates["jedha"] = go.layout.Template(
    layout_colorway=["#4B9AC7", "#4BE8E0", "#9DD4F3", "#97FBF6", "#2A7FAF", "#23B1AB", "#0E3449", "#015955"]
)
pio.templates.default = "jedha"
pio.renderers.default = "svg" # to be replaced by "iframe" if working on JULIE






df = pd.read_csv('bl.txt')


print("done")
df.head()





df.describe(include='all')


df['balance'].value_counts()






df.shape








# Separate target variable Y from features X
print("Separating labels from features...")
target_variable = "balance"

X = df.drop(target_variable, axis = 1)
Y = df.loc[:,target_variable]

print("...Done.")
print()

print('Y : ')
print(Y.head())
print()
print('X :')
print(X.head())






X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=42, stratify = Y)








### Preprocessings ###
# With a decision tree, there's no need to normalize the numerical variables !

# Encode label
encoder = LabelEncoder()
Y_train = encoder.fit_transform(Y_train)
Y_test = encoder.transform(Y_test)



classifier_gini = DecisionTreeClassifier(criterion="gini")
classifier_gini.fit(X_train, Y_train)






from sklearn.tree import plot_tree

class_names = encoder.inverse_transform([0, 1, 2])

plt.figure(figsize=(20,20))
a = plot_tree(classifier_gini,
              feature_names=X.columns,
              class_names=class_names,
              filled=True,
              rounded=True,
              fontsize=14)






classifier_entropy = DecisionTreeClassifier(criterion="entropy")
classifier_entropy.fit(X_train,Y_train)






plt.figure(figsize=(20,10))
a = plot_tree(classifier_entropy,
              feature_names=X.columns,
              class_names=class_names,
              filled=True,
              rounded=True,
              fontsize=14)






X_new = pd.DataFrame(data = [[4,4,3,3]], columns = ["left_weight", "left_distance", "right_weight", "right_distance"])
classifier_entropy.predict(X_new)



encoder.inverse_transform([1])



classifier_gini.predict(X_new)






y_pred_gini = classifier_gini.predict(X_test)
y_pred_gini






y_pred_entropy = classifier_entropy.predict(X_test)
y_pred_entropy









for item1, item2 in zip(y_pred_entropy, y_pred_gini):
    print(f"y_pred_entropy: {item1}, y_pred_gini: {item2}")


import numpy as np

# Boolean mask: True where predictions are identical
agree_mask = (y_pred_gini == y_pred_entropy)

count = (~agree_mask).sum()

print("on the test set, there are {} examples that have different predictions from Gini and Entropy".format(count))
print("Which corresponds to a percentage of {}% ".format(np.round(count / len(y_pred_gini) * 100, 3)))







classifier_gini.score(X_test, Y_test)






classifier_entropy.score(X_test, Y_test)






# Visualize confusion matrices
_ , ax = plt.subplots() # Get subplot from matplotlib
ax.set(title="Confusion Matrix on Test set") # Set a title that we will add into ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(classifier_entropy, X_test, Y_test, ax=ax) # ConfusionMatrixDisplay from sklearn
plt.show()











print(classification_report(y_true = Y_test, y_pred=classifier_entropy.predict(X_test)))



print(classification_report(y_true = Y_test, y_pred = classifier_gini.predict(X_test)))











































