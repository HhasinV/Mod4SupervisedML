{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e93df25-9ca4-4f7b-adc4-26cddacef879",
   "metadata": {},
   "source": [
    "## Inona mo ny principe ny ML\n",
    "- PrÃ©paration des donnÃ©es : **nettoyage, encoding, scaling**: OneHot, SimpleImputer, LabelEncoding, StandardScaler\n",
    "- Split train/test : 80/20 ou 70/30\n",
    "- EntraÃ®nement : **model.fit(X_train, y_train)**\n",
    "- PrÃ©diction : **y_pred = model.predict(X_test)**\n",
    "- Ã‰valuation avec mÃ©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed76982-3169-4426-a5b4-e63e7b6274b5",
   "metadata": {},
   "source": [
    "## Quels sont les differents metriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258e558-9992-4349-b7bf-900a767692bc",
   "metadata": {},
   "source": [
    "## REGRESSION LINEAIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901a48-2f37-4f67-87a3-50cc4f19f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORT Regression\n",
    "# ðŸ“Š 1. RÃ‰GRESSION (PrÃ©dire un NOMBRE)\n",
    "# Quand ?\n",
    "# PrÃ©dire valeur continue : prix, tempÃ©rature, quantitÃ©, Ã¢ge...\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 2. PRÃ‰PARATION DES DONNÃ‰ES\n",
    "# - GÃ©rer valeurs manquantes\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# - Encoder variables catÃ©gorielles (si nÃ©cessaire)\n",
    "# Pour rÃ©gression : souvent LabelEncoder ou OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "df['ville'] = le.fit_transform(df['ville'])\n",
    "\n",
    "# OU OneHotEncoder pour crÃ©er colonnes binaires\n",
    "df_encoded = pd.get_dummies(df, columns=['ville'], drop_first=True)\n",
    "\n",
    "# - SÃ©parer X et y\n",
    "X = df.drop('prix', axis=1)  # Features\n",
    "y = df['prix']                # Target (variable continue)\n",
    "\n",
    "# 3. SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. SCALING (Important pour rÃ©gression !)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Attention : transform seulement !\n",
    "\n",
    "# 5. ENTRAÃŽNEMENT\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. PRÃ‰DICTION\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Ã‰VALUATION\n",
    "print(f\"RÂ² : {r2_score(y_test, y_pred):.3f}\")  # 0 Ã  1 (proche de 1 = bon)\n",
    "print(f\"RMSE : {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\n",
    "print(f\"MAE : {mean_absolute_error(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecc4d4-7909-4386-bacc-d0f12f3f99e8",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 3. RÃ‰GRESSION LOGISTIQUE (Classification Binaire)\n",
    "Quand ?\n",
    "Classification binaire avec probabilitÃ©s : churn oui/non, clic oui/non..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e82dab-09bb-418a-9498-cdb59c8d42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORT\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# 2-3. PRÃ‰PARATION et SPLIT (identique Ã  classification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. SCALING (OBLIGATOIRE pour Logistic Regression !)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. ENTRAÃŽNEMENT\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. PRÃ‰DICTION\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # ProbabilitÃ©s classe 1\n",
    "\n",
    "# 7. Ã‰VALUATION\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"AUC-ROC : {roc_auc_score(y_test, y_proba):.3f}\")  # SpÃ©cifique logistique\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8455b24-8532-4525-a09a-730c24e665e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸŒ³ ARBRES DE DÃ‰CISION & FORÃŠTS ALÃ‰ATOIRES\n",
    "### 1. ARBRE DE DÃ‰CISION (Decision Tree)\n",
    "Concept simple\n",
    "C'est comme un organigramme de questions qui divise les donnÃ©es Ã©tape par Ã©tape.\n",
    "Exemple concret : PrÃ©dire si quelqu'un va acheter un produit\n",
    "                    Ã‚ge < 30 ans ?\n",
    "                   /              \\\n",
    "                OUI                NON\n",
    "                 /                  \\\n",
    "        Salaire > 2000â‚¬ ?      MariÃ© ?\n",
    "           /        \\           /     \\\n",
    "        OUI        NON        OUI    NON\n",
    "         /          \\          /       \\\n",
    "    ACHÃˆTE    ACHÃˆTE PAS   ACHÃˆTE  ACHÃˆTE PAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb2830-459d-4a30-b8c9-c53c82fa0d1d",
   "metadata": {},
   "source": [
    "### Comment Ã§a marche ?\n",
    "\n",
    "- L'algorithme cherche la meilleure question Ã  poser (feature qui sÃ©pare le mieux les classes)\n",
    "- Il divise les donnÃ©es selon la rÃ©ponse\n",
    "- Il rÃ©pÃ¨te le processus sur chaque sous-groupe\n",
    "- Il s'arrÃªte quand les groupes sont \"purs\" ou selon critÃ¨res dÃ©finis\n",
    "\n",
    "#### Avantages âœ…\n",
    "\n",
    "- Facile Ã  visualiser et interprÃ©ter\n",
    "- Pas besoin de scaling **(StandardScaler)**\n",
    "- GÃ¨re variables **catÃ©gorielles et numÃ©riques**\n",
    "- GÃ¨re les non-linÃ©aritÃ©s naturellement\n",
    "\n",
    "#### InconvÃ©nients âŒ\n",
    "\n",
    "- SURAPPRENTISSAGE (overfitting) trÃ¨s facile !\n",
    "- Instable : petits changements donnÃ©es â†’ arbre trÃ¨s diffÃ©rent\n",
    "- Performances moyennes seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d800f-d8e9-4dec-a078-98c63e21ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=5,           # Profondeur max (limite overfitting) â­\n",
    "    min_samples_split=10,  # Min Ã©chantillons pour diviser nÅ“ud\n",
    "    min_samples_leaf=5,    # Min Ã©chantillons par feuille\n",
    "    max_features='sqrt',   # Nombre features Ã  considÃ©rer\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "**ParamÃ¨tres clÃ©s Ã  retenir** :\n",
    "- **max_depth** : Plus important ! Limite la profondeur (3-10 gÃ©nÃ©ralement)\n",
    "- **min_samples_split** : Ã‰vite divisions sur petits groupes\n",
    "- **min_samples_leaf** : Ã‰vite feuilles trop spÃ©cifiques\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619007c-e92f-4fd9-99f5-cf9a92cca69d",
   "metadata": {},
   "source": [
    "\n",
    "## **2. FORÃŠT ALÃ‰ATOIRE (Random Forest)**\n",
    "\n",
    "### **Concept simple**\n",
    "C'est un **ensemble de nombreux arbres** qui votent ensemble !\n",
    "\n",
    "**Analogie** : Au lieu de demander Ã  1 expert, tu demandes Ã  100 experts et tu prends la rÃ©ponse majoritaire.\n",
    "```\n",
    "Arbre 1: ACHÃˆTE       â”‚\n",
    "Arbre 2: ACHÃˆTE       â”‚\n",
    "Arbre 3: ACHÃˆTE PAS   â”‚  VOTE MAJORITAIRE\n",
    "Arbre 4: ACHÃˆTE       â”‚  â†’ ACHÃˆTE (75%)\n",
    "...                   â”‚\n",
    "Arbre 100: ACHÃˆTE     â”‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffd200-d308-4c2f-bbd3-d36e08828771",
   "metadata": {},
   "source": [
    "#### Comment Ã§a marche ?\n",
    "\n",
    "- CrÃ©e plusieurs arbres (100, 500, 1000...)\n",
    "- Chaque arbre est entraÃ®nÃ© sur un Ã©chantillon alÃ©atoire des donnÃ©es **(bootstrap)**\n",
    "- Chaque arbre utilise un sous-ensemble alÃ©atoire de features Ã  chaque division\n",
    "- PrÃ©diction finale = vote majoritaire (classification) ou moyenne (rÃ©gression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7603d2-185e-47d9-b974-00ebb783ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,      # Nombre d'arbres (100-500) â­â­â­\n",
    "    max_depth=10,          # Profondeur max de chaque arbre â­\n",
    "    min_samples_split=5,   # Min Ã©chantillons pour diviser\n",
    "    min_samples_leaf=2,    # Min Ã©chantillons par feuille\n",
    "    max_features='sqrt',   # Features alÃ©atoires par division â­\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # Utilise tous les CPU (parallÃ©lisation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dee1fa-35ef-411d-90f7-7bfb50426547",
   "metadata": {},
   "outputs": [],
   "source": [
    "ParamÃ¨tres clÃ©s Ã  retenir :\n",
    "\n",
    "n_estimators : Plus = mieux (mais plus lent). Start avec 100.\n",
    "max_depth : ContrÃ´le overfitting (None par dÃ©faut, mais limiter Ã  10-20 souvent mieux)\n",
    "max_features : 'sqrt' pour classification, 'log2' alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af22290-f672-44ae-96ea-b963fc62d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== IMPORTS ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== DONNÃ‰ES EXEMPLE ==========\n",
    "# CrÃ©ons un dataset simple\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ========== 1. ARBRE DE DÃ‰CISION ==========\n",
    "print(\"=\" * 50)\n",
    "print(\"ARBRE DE DÃ‰CISION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ModÃ¨le\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=3,              # Limite profondeur (Ã©vite overfitting)\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# EntraÃ®nement\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Ã‰valuation\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred_dt):.3f}\")\n",
    "print(f\"F1-Score : {f1_score(y_test, y_pred_dt, average='weighted'):.3f}\")\n",
    "\n",
    "# Cross-validation pour vÃ©rifier overfitting\n",
    "cv_scores_dt = cross_val_score(dt_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Val Accuracy : {cv_scores_dt.mean():.3f} (+/- {cv_scores_dt.std():.3f})\")\n",
    "\n",
    "# Visualiser l'arbre (optionnel)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_model, \n",
    "          feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Arbre de DÃ©cision\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree.png')\n",
    "print(\"Arbre sauvegardÃ© dans 'decision_tree.png'\")\n",
    "\n",
    "# ========== 2. FORÃŠT ALÃ‰ATOIRE ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FORÃŠT ALÃ‰ATOIRE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ModÃ¨le\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,         # 100 arbres\n",
    "    max_depth=5,              # Profondeur max\n",
    "    min_samples_split=5,\n",
    "    max_features='sqrt',      # Features alÃ©atoires\n",
    "    random_state=42,\n",
    "    n_jobs=-1                 # ParallÃ©lisation\n",
    ")\n",
    "\n",
    "# EntraÃ®nement\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Ã‰valuation\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"F1-Score : {f1_score(y_test, y_pred_rf, average='weighted'):.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Val Accuracy : {cv_scores_rf.mean():.3f} (+/- {cv_scores_rf.std():.3f})\")\n",
    "\n",
    "# Rapport dÃ©taillÃ©\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=iris.target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# ========== 3. IMPORTANCE DES FEATURES ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"IMPORTANCE DES FEATURES (Random Forest)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualiser importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Importance des Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "print(\"\\nGraphique sauvegardÃ© dans 'feature_importance.png'\")\n",
    "\n",
    "# ========== 4. COMPARAISON ==========\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARAISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'ModÃ¨le': ['Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_dt),\n",
    "        accuracy_score(y_test, y_pred_rf)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_dt, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted')\n",
    "    ],\n",
    "    'CV Mean': [cv_scores_dt.mean(), cv_scores_rf.mean()],\n",
    "    'CV Std': [cv_scores_dt.std(), cv_scores_rf.std()]\n",
    "})\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad6d73-f881-4644-b8fd-72614fceaf7c",
   "metadata": {},
   "source": [
    "5. Ã‰LÃ‰MENTS Ã€ CONNAÃŽTRE ABSOLUMENT\n",
    "Pour Arbre de DÃ©cision ðŸŒ³\n",
    "\n",
    "max_depth : TOUJOURS limiter (3-10) sinon overfitting garanti\n",
    "Pas besoin de StandardScaler\n",
    "Facilement visualisable avec plot_tree()\n",
    "Bon pour comprendre les donnÃ©es, moins pour production\n",
    "\n",
    "Pour Random Forest ðŸŒ²ðŸŒ²ðŸŒ²\n",
    "\n",
    "n_estimators : Plus = mieux (100 minimum, 500 optimal souvent)\n",
    "max_depth : Limiter aussi (10-20) mÃªme si moins critique\n",
    "feature_importances_ : Indique quelles features sont importantes\n",
    "Out-of-bag (OOB) score : Validation automatique sans split\n",
    "Meilleur choix pour la plupart des problÃ¨mes rÃ©els\n",
    "\n",
    "CritÃ¨res de division (thÃ©orie)\n",
    "\n",
    "Gini : Par dÃ©faut, mesure impuretÃ© des nÅ“uds\n",
    "Entropy : Alternative (information gain)\n",
    "Ne pas trop s'en soucier, Gini fonctionne bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea061fb9-ab3f-41b0-90b3-a08ff271bdcf",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5ebeb-d4a5-4819-8eb6-ca7bc5e18633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Toujours faire cross-validation pour dÃ©tecter overfitting\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"CV: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
    "\n",
    "# 2. GridSearchCV pour trouver meilleurs paramÃ¨tres\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Meilleurs params: {grid.best_params_}\")\n",
    "\n",
    "# 3. Feature importance pour feature selection\n",
    "importances = rf_model.feature_importances_\n",
    "# Garder seulement features importantes (> seuil)\n",
    "important_features = X.columns[importances > 0.05]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
