{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce43d17-48c4-4ae9-8bcf-4b3031364d66",
   "metadata": {},
   "source": [
    "- Imputation des valeurs\n",
    "- Standardisation /Normalisation (atao meme echele)\n",
    "- Encodage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71940af3-d2e4-4ef2-ba31-ab7cddd8378d",
   "metadata": {},
   "source": [
    "Exercice Ã  faire: preprocessing advanced\n",
    "- A revoir: https://app.jedha.co/course/preprocessing-with-sklearn-course-ft/what-is-preprocessing-ft\n",
    "- Pandas: https://app.jedha.co/course/pandas-statistics-ft/data-manipulation-with-pandas-ft\n",
    "- L'imputation en apprentissage automatique consiste Ã  remplacer les valeurs manquantes par des donnÃ©es valides: Misy ny Imputation par moyenne, par mediane ary ho an'ny variable qualitative dia imputation par mode (Izay miverimberina matetika)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d3a95-460c-4706-9351-5e39aa88e14c",
   "metadata": {},
   "source": [
    "#### Colonnes Ã  supprimer\n",
    "- Toutes les colonnes qui constituent des Â« identifiants uniques Â»\n",
    "- Colonnes prÃ©sentant un nombre excessif de valeurs manquantes.\n",
    "- Les variables nominales prÃ©sentant un trop grand nombre de modalitÃ©s doivent Ãªtre exclues.\n",
    "- Si deux colonnes sont colinÃ©aires (coefficient de corrÃ©lation Ã©gal Ã  1), une seule sera conservÃ©e : en rÃ¨gle gÃ©nÃ©rale, il ne faut jamais conserver Ã  la fois lâ€™Ã¢ge et lâ€™annÃ©e de naissance dâ€™une personne dans un ensemble de donnÃ©es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44eace0-ac4d-427a-b2ed-f90098ee0396",
   "metadata": {},
   "source": [
    "Apprentissage superivisÃ©: efa nisy exemple ka hitady ny manaraka\n",
    "- Regression: Rehefa VA quantitative\n",
    "- Classification: Rehefa VA qulaitative\n",
    "- Y = f(x) + e: Y: valaur cible/ Valeur depentende"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03f551-733d-494c-9bd1-2b34cc4aab6e",
   "metadata": {},
   "source": [
    "### Standardisation/Normalisation (variables quantitatives uniquement) ðŸ“Š\n",
    "- Mba tsy himanipulena valeur avo be (deconeillÃ© en informatique satria mihinana ressources), et pour faciliter l'entraÃ®nement de nos modÃ¨les, nous normaliserons toutes les variables explicatives quantitatives afin que leurs valeurs soient comprises dans des intervalles raisonnables, gÃ©nÃ©ralement de quelques unitÃ©s\n",
    "- . Il existe diffÃ©rentes mÃ©thodes de normalisation des donnÃ©es, mais la plus courante est la standardisation : la variable\n",
    "\n",
    "Xest remplacÃ© parâ€‹\n",
    " , nous supprimons la moyenne de la variable et divisons par l'Ã©cart type de la variable, ce qui donne une variable de moyenne 0 et d'Ã©cart type 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9af606-a295-4502-87cb-dc8f410c7ca6",
   "metadata": {},
   "source": [
    "### Encodage\n",
    "- Enfin, comme les modÃ¨les d'apprentissage automatique ne traitent que des nombres, toutes les variables catÃ©gorielles doivent Ãªtre encodÃ©es, c'est-Ã -dire transformÃ©es en nombres. Selon le type de variable, diffÃ©rentes mÃ©thodes seront utilisÃ©es :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a9526-bcac-49be-b772-7e888a3885f4",
   "metadata": {},
   "source": [
    "#### Variable Cible = Variable Explicative + Epsilon\n",
    "- Variable cible = Encodena, avadika nombre fotsiny ilay char ohatraa e\n",
    "- Variable explicative = mety ho **nominal**(Raha nominale dia mety afaka atao otrany variable cible, soloina nombre fotsiny), mety ho **ordinal** (inty tsy mety soloina fotsiny fa tsy maintsy atao matrice satria misy ny hierarchie, dia inty matetika no ampiasana ilay **ONEHOT** na one-hot (ou encodage binaire)) ,mety ho **nombre** fa efa **Standardisena** na **Normalisena** mba ho meme echelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fcd61a-3fda-4b35-8740-fd5ea07a1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rehefa mamafa colonne, axis = 1 midika fa fafaina koa ny ligne rehetra miaaraka aminy\n",
    "# Drop useless columns / columns with too many missing values\n",
    "useless_cols = [\"id\", \"useless_col\", \"almost_empty\"]\n",
    "\n",
    "print(\"Dropping useless columns...\")\n",
    "dataset = dataset.drop(\n",
    "    useless_cols, axis=1\n",
    ")  # axis = 1 indicates that we are dropping along the column axis\n",
    "# never hesitate to look at a function's documentation using the command name_of_the_function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1d570-4561-4512-a5ef-f341ea939ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines containing outliers (using masks)\n",
    "\n",
    "print(\"Dropping outliers in Age...\")\n",
    "to_keep = (dataset[\"Age\"] > 0) | (\n",
    "    dataset[\"Age\"].isnull()\n",
    ")  # We want keeping positives values or missings\n",
    "dataset = dataset.loc[to_keep, :]\n",
    "print(\"Done. Number of lines remaining : \", dataset.shape[0])\n",
    "print()\n",
    "\n",
    "print(\"Dropping outliers in Salary...\")\n",
    "to_keep = dataset[\"Salary\"] < dataset[\"Salary\"].mean() + 2 * dataset[\"Salary\"].std()\n",
    "dataset = dataset.loc[to_keep, :]\n",
    "print(\"Done. Number of lines remaining : \", dataset.shape[0])\n",
    "print()\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aa4c1-d026-42ee-96c8-ae5ea3a5a2f4",
   "metadata": {},
   "source": [
    "### SÃ©paration des groupes d'entraÃ®nement et de test\n",
    "- Avant tout prÃ©traitement, mettons de cÃ´tÃ© une petite partie (10 Ã  20 %) de l'ensemble de donnÃ©es. Ces donnÃ©es constitueront l' ensemble de test et ne serviront pas Ã  l'entraÃ®nement du modÃ¨le. Plus loin dans ce notebook, nous utiliserons la plus grande partie (80 Ã  90 %) des donnÃ©es (l' ensemble d'entraÃ®nement ) pour entraÃ®ner notre modÃ¨le. L'ensemble de test nous permettra alors d'estimer les performances attendues sur de nouvelles donnÃ©es jamais vues par le modÃ¨le.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ec8d7-b2a6-4404-9fcd-54a18bedb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59059d0-9012-4239-a531-caca01b12f00",
   "metadata": {},
   "source": [
    "### PrÃ©traitements : imputation des valeurs manquantes, standardisation et encodage one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e782a-bea3-4ce9-824a-c194e727e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_features = [\"Age\", \"Salary\"]  # Names of numeric columns in X_train/X_test\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"median\"),\n",
    "        ),  # missing values will be replaced by columns' median\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = [\"Country\"]  # Names of categorical columns in X_train/X_test\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"most_frequent\"),\n",
    "        ),  # missing values will be replaced by most frequent value\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OneHotEncoder(drop=\"first\"),\n",
    "        ),  # first column will be dropped to avoid creating correlations between features\n",
    "    ]\n",
    ")\n",
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949fe30-48d2-4177-a828-e1eeaa8e3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# 1ï¸âƒ£ Vos donnÃ©es\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 30, None, 40],\n",
    "    'salaire': [30000, 40000, 50000, 60000],\n",
    "    'ville': ['Paris', 'Lyon', 'Nice', 'Paris'],\n",
    "    'diplome': ['Bac', 'Master', None, 'Doctorat']\n",
    "})\n",
    "\n",
    "# 2ï¸âƒ£ DÃ©finir quelles colonnes sont de quel type\n",
    "numeric_features = ['age', 'salaire']  # Colonnes numÃ©riques\n",
    "categorical_features = ['ville', 'diplome']  # Colonnes catÃ©gorielles\n",
    "\n",
    "# 3ï¸âƒ£ DÃ©finir les transformations pour les colonnes numÃ©riques\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Remplir les NaN\n",
    "    ('scaler', StandardScaler())  # Standardiser\n",
    "])\n",
    "\n",
    "# 4ï¸âƒ£ DÃ©finir les transformations pour les colonnes catÃ©gorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Remplir les NaN\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # Encoder\n",
    "])\n",
    "\n",
    "# 5ï¸âƒ£ CrÃ©er le ColumnTransformer (votre code !)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        # â†‘ nom    â†‘ quoi faire           â†‘ sur quelles colonnes\n",
    "        \n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        # â†‘ nom    â†‘ quoi faire              â†‘ sur quelles colonnes\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ Appliquer toutes les transformations en une fois !\n",
    "X_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "print(\"Forme originale :\", df.shape)  # (4, 4)\n",
    "print(\"Forme transformÃ©e :\", X_transformed.shape)  # (4, 8)\n",
    "# 2 colonnes num + 6 colonnes cat (OneHot a crÃ©Ã© plusieurs colonnes)\n",
    "```\n",
    "\n",
    "## Visualisation du processus\n",
    "```\n",
    "DataFrame original\n",
    "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ age â”‚ salaire â”‚ ville  â”‚ diplome  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 25  â”‚ 30000   â”‚ Paris  â”‚ Bac      â”‚\n",
    "â”‚ 30  â”‚ 40000   â”‚ Lyon   â”‚ Master   â”‚\n",
    "â”‚ NaN â”‚ 50000   â”‚ Nice   â”‚ NaN      â”‚\n",
    "â”‚ 40  â”‚ 60000   â”‚ Paris  â”‚ Doctorat â”‚\n",
    "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â”‚\n",
    "          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚                â”‚                 â”‚\n",
    "          â–¼                â–¼                 â–¼\n",
    "    numeric_features   categorical_features\n",
    "    [age, salaire]     [ville, diplome]\n",
    "          â”‚                â”‚\n",
    "          â–¼                â–¼\n",
    "  numeric_transformer  categorical_transformer\n",
    "          â”‚                â”‚\n",
    "          â”œâ”€ Imputer       â”œâ”€ Imputer\n",
    "          â””â”€ Scaler        â””â”€ OneHotEncoder\n",
    "          â”‚                â”‚\n",
    "          â–¼                â–¼\n",
    "    [age_scaled,      [ville_Lyon, ville_Nice,\n",
    "     salaire_scaled]  diplome_Doctorat, diplome_Master]\n",
    "          â”‚                â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â–¼\n",
    "          X_transformed (array numpy)\n",
    "      [toutes les colonnes combinÃ©es]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bbcfe-824b-4c33-af3d-f31e4941b8e2",
   "metadata": {},
   "source": [
    "# Regression linaire\n",
    "-  les modÃ¨les de rÃ©gression: Natao ho ana problÃ¨me continue, izany hoe variable quantitative continue c'est-Ã -dire tout modÃ¨le d'apprentissage automatique permettant d'estimer une variable cible numÃ©rique et continue .\n",
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8c08f-399e-4cdf-9e6e-677da8d70a9a",
   "metadata": {},
   "source": [
    "coment trouvr a et b: Moindre carrÃ©s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e6f68-9035-443a-8f04-1d3e09e4af95",
   "metadata": {},
   "source": [
    "Rehefa traran'ny overfit dia mila amboarina ny modÃ¨le: Ridge et Lasso sont deux variantes de la technique de rÃ©gularisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23756ff-53d3-4787-b8f9-6668d6c42988",
   "metadata": {},
   "source": [
    "### A voir\n",
    "analyse bivariÃ©\n",
    "https://app.jedha.co/course/linear-regression-ft/linear-regressions-ft\n",
    "\n",
    "### Indicateur de position:\n",
    "- Moyenne: moyenne ponderÃ©e, moyenne arthimetique, moyenne harmonique\n",
    "- Mediane\n",
    "\n",
    "### Indicateur de dispersion\n",
    "- Fanotaniana majeur: mety manodidina ny haotrinona eo ny salairen'ny dev ao @ departement (Azo tsoahina zany aloha ny **moyenne** an'ny salaire (indicateur de position), ilay hoe mety manodidina ny haotrinona dia io ndray dia indicateur de dispersion. Izay no tedavina @ **variance**: **distance an le variable 1 @ moyenne le tou au carrÃ©**\n",
    "- **Variance** zany: distance entre le varibale et la moyenne (X-Xbar)/n au carrÃ© zany\n",
    "- **Ecart type**: dia variance iany fa normalisena (betsaka ny antony): racie carrÃ© ny variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e43ab-4f5b-4103-bd41-723837f40192",
   "metadata": {},
   "source": [
    "# Exerice Weekly challanges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
